{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from hashlib import sha256\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transaction:\n",
    "\tdef __init__(self, time, txid, withdraw, deposit, amount):\n",
    "\t\tself.time = time\n",
    "\t\tself.txid = txid\n",
    "\t\tself.withdraw = withdraw\n",
    "\t\tself.deposit = deposit\n",
    "\t\tself.amount = amount\n",
    "\n",
    "class partial_block:\n",
    "    def __init__(self, parenthash, height, miner_ip, miner_port):\n",
    "        self.parenthash = parenthash\n",
    "        self.txs = []\n",
    "        self.height = height\n",
    "        self.miner_ip= miner_ip\n",
    "        self.miner_port = miner_port\n",
    "    def blockhash(self):\n",
    "        return sha256(pickle.dumps(self)).hexdigest()\n",
    "\n",
    "class block:\n",
    "    def __init__(self, nonce, time, partial_block):\n",
    "    \tself.time = time\n",
    "    \tself.nonce = nonce\n",
    "    \tself.partial_block = partial_block\n",
    "\n",
    "class blockchain:\n",
    "    def __init__(self, genesis_hash):\n",
    "        self.blocks = {} # Block:Parent\n",
    "        self.blockheight = {} # Blockhash: height \n",
    "        self.blocks[genesis_hash] = 0\n",
    "        self.head = genesis_hash # Hash of the head block\n",
    "        self.longest_chain = [genesis_hash] #List of blockhashes on the longest chain\n",
    "        self.len = 1\n",
    "        self.tx_included = {} #TXID: TX at the head\n",
    "        self.blockdb = {} #Blockhash to block\n",
    "        self.state_map = {genesis_hash:{}} #Block hash to state mapping\n",
    "        self.time_processed = {} #Blockhash: time\n",
    "    def insert(self, block):\n",
    "        blockhash = block.partial_block.blockhash()\n",
    "        parenthash = block.partial_block.parenthash\n",
    "        if parenthash not in self.blocks.keys():\n",
    "            return \"orphan_block\"\n",
    "        height = block.partial_block.height\n",
    "        # Update Statemap\n",
    "        temp_state = self.state_map[parenthash].copy()\n",
    "        (temp_state,validity_bool) = stf(temp_state,block.partial_block.txs)\n",
    "        if not validity_bool:\n",
    "            return \"invalid_block\"\n",
    "        self.state_map[blockhash] = temp_state\n",
    "        #Updatestatemap complete\n",
    "        self.blocks[blockhash] = parenthash\n",
    "        self.blockheight[blockhash] = height\n",
    "        self.blockdb[blockhash] = block\n",
    "        self.time_processed[blockhash] = time.time()\n",
    "        if self.head == parenthash: # The normal scenario\n",
    "            self.head = blockhash\n",
    "            self.len = self.len + 1\n",
    "            self.longest_chain.append(blockhash)\n",
    "            #update tx_included\n",
    "            for tx in block.partial_block.txs:\n",
    "                self.tx_included[tx.txid] = tx\n",
    "            #update tx_included complete\n",
    "        elif height > self.len: #CAUTION: Do not do blockchain.insert for orphan blocks\n",
    "            connected_ancestor = parenthash\n",
    "            self.head = blockhash\n",
    "            self.len = height\n",
    "            # Now update the longest chain\n",
    "            new_suffix = [parenthash,blockhash]\n",
    "            while connected_ancestor not in self.longest_chain:\n",
    "                connected_ancestor = self.blocks[connected_ancestor] # Look for previous generation\n",
    "                new_suffix = [connected_ancestor] + new_suffix\n",
    "            ca_height = self.blockheight[connected_ancestor]\n",
    "            removed_suffix = self.longest_chain[(ca_height-1):]\n",
    "            self.longest_chain = self.longest_chain[:(ca_height-1)]\n",
    "            self.longest_chain = self.longest_chain + new_suffix\n",
    "            #update tx_included\n",
    "            for removed_bh in removed_suffix:\n",
    "                removed_blk = self.blockdb[removed_bh]\n",
    "                for tx in removed_blk.partial_block.txs:\n",
    "                    self.tx_included.pop(tx.txid,None)\n",
    "            for added_bh in new_suffix:\n",
    "                added_blk = self.blockdb[added_bh]\n",
    "                for tx in added_blk.partial_block.txs:\n",
    "                    self.tx_included[tx.txid] = tx\n",
    "            #update tx_included complete\n",
    "        else:\n",
    "            pass\n",
    "        return \"block_processed\"\n",
    "    def head_state(self):\n",
    "        return self.state_map[self.head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "NEWOBJ class argument isn't a type object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-31bb5a77a955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/ranvirrana/Downloads/sp2020/mp2/blockchain1.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m: NEWOBJ class argument isn't a type object"
     ]
    }
   ],
   "source": [
    "file = open(\"/Users/ranvirrana/Downloads/sp2020/mp2/blockchain1.pkl\", 'rb')\n",
    "chain = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print(len(chain.longest_chain))\n",
    "print(len(chain.blockdb.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00847983360291\n"
     ]
    }
   ],
   "source": [
    "block_delay_dict = {}\n",
    "block_delay_array = []\n",
    "for blockhash in chain.blockdb:\n",
    "    block = chain.blockdb[blockhash]\n",
    "    creation_time = float(block.time)\n",
    "    rcvd_time = chain.time_processed[blockhash]\n",
    "    delay = rcvd_time - creation_time\n",
    "    block_delay_dict[blockhash] = delay\n",
    "    block_delay_array.append(delay)\n",
    "#print(block_delay_dict)\n",
    "block_delay_array = np.array(block_delay_array)\n",
    "block_delay_mean = np.mean(block_delay_array)\n",
    "print(np.max(block_delay_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.18324637608\n"
     ]
    }
   ],
   "source": [
    "transaction_delay_dict = {}\n",
    "transaction_delay_array = []\n",
    "for blockhash in chain.longest_chain:\n",
    "    if blockhash != '24c04d87a97bdcefc011d2dd182eba1beaedfe91ae22e632ffb73e5ae7086bed':\n",
    "        block = chain.blockdb[blockhash]\n",
    "        creation_time = float(block.time)\n",
    "        for tx in block.partial_block.txs:\n",
    "            transaction_time = float(tx.time)\n",
    "            delay = creation_time - transaction_time\n",
    "            transaction_delay_dict[tx.txid] = delay\n",
    "            transaction_delay_array.append(delay)\n",
    "        \n",
    "transaction_delay_array = np.array(transaction_delay_array)\n",
    "transaction_delay_mean = np.mean(transaction_delay_array)\n",
    "print(np.mean(transaction_delay_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "blockparent = chain.blocks\n",
    "parentblock = {} \n",
    "num_forks = 0\n",
    "fork_childs = []\n",
    "for block in blockparent:\n",
    "    parent = blockparent[block]\n",
    "    if parent not in parentblock.keys():\n",
    "        parentblock[parent] = [block]\n",
    "    else:\n",
    "        parentblock[parent] = parentblock[parent] + [block]\n",
    "        num_forks += 1\n",
    "        fork_childs.append(block)\n",
    "print(num_forks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'55d9a20296fc088cbf47d99a59188c579c891b12e1ded5edad6b011c9e720028': 1, '821df563b6f032c5a2d32609d306f906dcfbd54f006f57f6ae1482bb7b525069': 1, '66bf47861987f2444d3b6fe4f675f64466ea309db110a761196664baec62676a': 1, '21991d5246d0bf79a345d99aab42567431f5b05b240fbe86c14b650aebd13d11': 1, 'a268fc56566fd38cae78b840dca9f1eef7d4a595809d1b3a3f0d2e2f76512234': 1}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "fork_len_dict = {} #from basis blocks\n",
    "def ancestor_in_lc(blockhash,distance):\n",
    "    parenthash = blockparent[blockhash]\n",
    "    distance +=1\n",
    "    if parenthash not in chain.longest_chain:\n",
    "        distance = ancestor_in_lc(parenthash,distance)\n",
    "    return(distance)\n",
    "for blockhash in fork_childs:\n",
    "    fork_len_dict[blockhash] = ancestor_in_lc(blockhash,0)\n",
    "print(fork_len_dict)\n",
    "longest_fork = max(fork_len_dict.values())\n",
    "print(longest_fork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest chain = 45\n",
      "Total number of blocks = 50\n",
      "Average block delay =0.00337188584464\n",
      "Average transaction delay =9.18324637608\n",
      "Number of forks: 5\n",
      "Longest fork length: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of longest chain = \" + str(len(chain.longest_chain)))\n",
    "print(\"Total number of blocks = \" + str(len(chain.blockdb.keys())+1))\n",
    "\n",
    "\n",
    "#BLOCK DELAY STORED IN block_delay_array \n",
    "block_delay_dict = {}\n",
    "block_delay_array = []\n",
    "for blockhash in chain.blockdb:\n",
    "    block = chain.blockdb[blockhash]\n",
    "    creation_time = float(block.time)\n",
    "    rcvd_time = chain.time_processed[blockhash]\n",
    "    delay = rcvd_time - creation_time\n",
    "    block_delay_dict[blockhash] = delay\n",
    "    block_delay_array.append(delay)\n",
    "#print(block_delay_dict)\n",
    "block_delay_array = np.array(block_delay_array)\n",
    "block_delay_mean = np.mean(block_delay_array)\n",
    "print(\"Average block delay =\" + str(np.mean(block_delay_array)))\n",
    "\n",
    "\n",
    "#TRANSACTION DELAY STORED IN transaction_delay_array \n",
    "transaction_delay_dict = {}\n",
    "transaction_delay_array = []\n",
    "for blockhash in chain.longest_chain:\n",
    "    if blockhash != '24c04d87a97bdcefc011d2dd182eba1beaedfe91ae22e632ffb73e5ae7086bed':\n",
    "        block = chain.blockdb[blockhash]\n",
    "        creation_time = float(block.time)\n",
    "        for tx in block.partial_block.txs:\n",
    "            transaction_time = float(tx.time)\n",
    "            delay = creation_time - transaction_time\n",
    "            transaction_delay_dict[tx.txid] = delay\n",
    "            transaction_delay_array.append(delay)\n",
    "        \n",
    "transaction_delay_array = np.array(transaction_delay_array)\n",
    "transaction_delay_mean = np.mean(transaction_delay_array)\n",
    "print(\"Average transaction delay =\" + str(np.mean(transaction_delay_array)))\n",
    "\n",
    "\n",
    "# NUMBER OF FORKS STORED IN num_forks\n",
    "blockparent = chain.blocks\n",
    "parentblock = {} \n",
    "num_forks = 0\n",
    "fork_childs = []\n",
    "for block in blockparent:\n",
    "    parent = blockparent[block]\n",
    "    if parent not in parentblock.keys():\n",
    "        parentblock[parent] = [block]\n",
    "    else:\n",
    "        parentblock[parent] = parentblock[parent] + [block]\n",
    "        num_forks += 1\n",
    "        fork_childs.append(block)\n",
    "print(\"Number of forks: \" + str(num_forks))\n",
    "\n",
    "\n",
    "#LONGEST FORK STORED IN longest_fork\n",
    "fork_len_dict = {} #from basis blocks\n",
    "def ancestor_in_lc(blockhash,distance):\n",
    "    parenthash = blockparent[blockhash]\n",
    "    distance +=1\n",
    "    if parenthash not in chain.longest_chain:\n",
    "        distance = ancestor_in_lc(parenthash,distance)\n",
    "    return(distance)\n",
    "for blockhash in fork_childs:\n",
    "    fork_len_dict[blockhash] = ancestor_in_lc(blockhash,0)\n",
    "longest_fork = max(fork_len_dict.values())\n",
    "print(\"Longest fork length: \" + str(longest_fork))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
